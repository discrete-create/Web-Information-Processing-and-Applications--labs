{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†æ„å»ºæµç¨‹å°è£…æˆå‡½æ•°ï¼Œä¾¿äºè„šæœ¬æ‰§è¡Œ\n",
    "from pathlib import Path\n",
    "\n",
    "def run_part2_build(forward_file_path=\"normalized_tokens.json\", out_path=\"inverted_index.json\"):\n",
    "    if not Path(forward_file_path).exists():\n",
    "        raise FileNotFoundError(f\"æœªæ‰¾åˆ°æ­£æ’è¡¨æ–‡ä»¶: {forward_file_path}\")\n",
    "    index = ImprovedInvertedIndex()\n",
    "    ok = index.build_index_from_file(forward_file_path)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"æ„å»ºå¤±è´¥ï¼šæ— æ³•ä»JSONåŠ è½½æ–‡æ¡£\")\n",
    "    index.print_index_statistics()\n",
    "    index.save_inverted_index(out_path)\n",
    "    return out_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ä½ç½®ç´¢å¼•ï¼šåœ¨å€’æ’è¡¨ä¸­åŠ å…¥è¯é¡¹çš„ä½ç½®ä¿¡æ¯\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_positional_index_from_json(forward_file_path: str):\n",
    "    \"\"\"ä»æ­£æ’è¡¨(æ–‡ä»¶->tokensåˆ—è¡¨)æ„å»ºå¸¦ä½ç½®ä¿¡æ¯çš„å€’æ’ç´¢å¼•ã€‚\n",
    "    è¿”å›: vocabulary(è¯->id), postings(termId -> [{doc, tf, pos:[...]}])\n",
    "    \"\"\"\n",
    "    import json\n",
    "    with open(forward_file_path, 'r', encoding='utf-8') as f:\n",
    "        token_dict = json.load(f)\n",
    "\n",
    "    # è¯æ±‡è¡¨\n",
    "    all_terms = sorted({t for tokens in token_dict.values() for t in tokens})\n",
    "    vocabulary = {t: i for i, t in enumerate(all_terms)}\n",
    "\n",
    "    # termId -> docId -> positions\n",
    "    positions_map = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, tokens in token_dict.items():\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            tid = vocabulary[tok]\n",
    "            positions_map[tid][doc_id].append(idx)\n",
    "\n",
    "    # ç”Ÿæˆå€’æ’ç»“æ„ï¼ˆä¸åŠ è·³è¡¨ï¼Œåªæ¼”ç¤ºä½ç½®ä¿¡æ¯ï¼‰\n",
    "    postings = {}\n",
    "    for tid, doc2pos in positions_map.items():\n",
    "        items = []\n",
    "        for doc_id, pos_list in doc2pos.items():\n",
    "            items.append({\"doc\": doc_id, \"tf\": len(pos_list), \"skip\": -1, \"pos\": pos_list})\n",
    "        # ä¾æ®æ„å»ºæ—¶çš„æ–‡æ¡£åºä¿è¯é¡ºåºç¨³å®š\n",
    "        items.sort(key=lambda x: x[\"doc\"]) \n",
    "        postings[tid] = items\n",
    "\n",
    "    return vocabulary, postings\n",
    "\n",
    "\n",
    "def save_positional_index(vocabulary, postings, out_path: str):\n",
    "    import json\n",
    "    data = {\"vocabulary\": vocabulary, \"postings\": postings}\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"å¸¦ä½ç½®ä¿¡æ¯çš„å€’æ’ç´¢å¼•å·²ä¿å­˜: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb579b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) è¯å…¸å‹ç¼©ï¼šæŒ‰å—å­˜å‚¨ï¼ˆBlocking, k=4ï¼‰ä¸å‰ç«¯ç¼–ç ï¼ˆFront Codingï¼‰\n",
    "from typing import List, Tuple\n",
    "\n",
    "def encode_blocking(terms: List[str], k: int = 4) -> Tuple[bytes, List[int]]:\n",
    "    \"\"\"ç®€åŒ–ç‰ˆæŒ‰å—å­˜å‚¨ç¼–ç ã€‚\n",
    "    è¾“å‡º: ç¼–ç åçš„å­—èŠ‚åºåˆ—ã€å—èµ·å§‹åç§»åˆ—è¡¨ã€‚\n",
    "    è§„åˆ™ï¼š\n",
    "    - æ¯ä¸ªå—çš„ç¬¬ä¸€ä¸ªè¯ï¼šå†™å…¥ 1å­—èŠ‚é•¿åº¦ + åŸè¯å­—èŠ‚\n",
    "    - å—å†…å…¶ä½™è¯ï¼šå†™å…¥ 1å­—èŠ‚é•¿åº¦ + åŸè¯å­—èŠ‚ï¼ˆæ¼”ç¤ºç”¨é€”ï¼Œç°å®å¯ç»“åˆå‰ç¼€å·®åˆ†ï¼‰\n",
    "    - è®°å½•æ¯å—èµ·å§‹åœ¨å­—èŠ‚æµä¸­çš„åç§»\n",
    "    æ³¨ï¼šè¿™é‡Œä½¿ç”¨utf-8ç¼–ç ï¼Œé•¿åº¦ä»¥å•å­—èŠ‚ä¿å­˜ï¼Œé€‚ç”¨äºè‹±æ–‡å°å†™ç¤ºä¾‹ã€‚\n",
    "    \"\"\"\n",
    "    buf = bytearray()\n",
    "    offsets = []\n",
    "    for i, term in enumerate(terms):\n",
    "        if i % k == 0:\n",
    "            offsets.append(len(buf))\n",
    "        b = term.encode('utf-8')\n",
    "        l = len(b)\n",
    "        buf.append(l if l < 256 else 255)  # ç®€åŒ–ï¼š>255æˆªæ–­\n",
    "        buf.extend(b)\n",
    "    return bytes(buf), offsets\n",
    "\n",
    "\n",
    "def encode_front_coding(terms: List[str], k: int = 4) -> Tuple[bytes, List[int]]:\n",
    "    \"\"\"å‰ç«¯ç¼–ç ï¼šæ¯å—è®°å½•ç¬¬ä¸€ä¸ªè¯åŸæ ·ï¼Œåç»­è¯å­˜å…¬å…±å‰ç¼€é•¿åº¦+åç¼€ã€‚\n",
    "    çº¦å®šï¼š\n",
    "    - å—é¦–ï¼š1å­—èŠ‚é•¿åº¦ + åŸè¯\n",
    "    - å…¶ä½™ï¼š1å­—èŠ‚å…¬å…±å‰ç¼€é•¿åº¦ + 1å­—èŠ‚åç¼€é•¿åº¦ + åç¼€å­—èŠ‚\n",
    "    è¿”å›ï¼šç¼–ç å­—èŠ‚ã€å—åç§»ã€‚\n",
    "    \"\"\"\n",
    "    buf = bytearray()\n",
    "    offsets = []\n",
    "    for i in range(0, len(terms)):\n",
    "        if i % k == 0:\n",
    "            offsets.append(len(buf))\n",
    "            head = terms[i].encode('utf-8')\n",
    "            buf.append(len(head) if len(head) < 256 else 255)\n",
    "            buf.extend(head)\n",
    "            base = terms[i]\n",
    "        else:\n",
    "            t = terms[i]\n",
    "            # å…¬å…±å‰ç¼€\n",
    "            p = 0\n",
    "            while p < min(len(base), len(t)) and base[p] == t[p]:\n",
    "                p += 1\n",
    "            suffix = t[p:].encode('utf-8')\n",
    "            buf.append(p if p < 256 else 255)\n",
    "            buf.append(len(suffix) if len(suffix) < 256 else 255)\n",
    "            buf.extend(suffix)\n",
    "    return bytes(buf), offsets\n",
    "\n",
    "\n",
    "def measure_vocab_and_compression(vocabulary: dict, k: int = 4) -> dict:\n",
    "    terms_sorted = sorted(vocabulary.keys())\n",
    "    # åŸå§‹ï¼šç›´æ¥ä¸²è”+é•¿åº¦\n",
    "    raw_bytes = bytearray()\n",
    "    for t in terms_sorted:\n",
    "        b = t.encode('utf-8')\n",
    "        raw_bytes.append(len(b) if len(b) < 256 else 255)\n",
    "        raw_bytes.extend(b)\n",
    "    raw_size = len(raw_bytes)\n",
    "\n",
    "    blk_bytes, blk_offsets = encode_blocking(terms_sorted, k)\n",
    "    fc_bytes, fc_offsets = encode_front_coding(terms_sorted, k)\n",
    "\n",
    "    return {\n",
    "        \"vocab_size\": len(terms_sorted),\n",
    "        \"raw_size\": raw_size,\n",
    "        \"blocking\": {\"k\": k, \"bytes\": len(blk_bytes), \"blocks\": len(blk_offsets)},\n",
    "        \"front_coding\": {\"k\": k, \"bytes\": len(fc_bytes), \"blocks\": len(fc_offsets)}\n",
    "    }\n",
    "\n",
    "\n",
    "def save_compression_stats(stats: dict, out_path: str):\n",
    "    import json\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"å‹ç¼©ç»Ÿè®¡å·²ä¿å­˜: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) ä¸€é”®è¿è¡Œï¼šç”Ÿæˆ positional ç´¢å¼•ä¸å‹ç¼©ç»Ÿè®¡\n",
    "forward = \"normalized_tokens.json\"\n",
    "pos_out = \"inverted_index_pos.json\"\n",
    "comp_out = \"compression_stats.json\"\n",
    "\n",
    "vocab, postings = build_positional_index_from_json(forward)\n",
    "save_positional_index(vocab, postings, pos_out)\n",
    "\n",
    "stats = measure_vocab_and_compression(vocab, k=4)\n",
    "save_compression_stats(stats, comp_out)\n",
    "\n",
    "print(\"å®Œæˆï¼š\\n - å¸¦ä½ç½®ä¿¡æ¯ç´¢å¼•:\", pos_out, \"\\n - å‹ç¼©ç»Ÿè®¡:\", comp_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f0bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å€’æ’ç´¢å¼•æ„å»ºç³»ç»Ÿ - ä»JSONæ­£æ’è¡¨æ–‡ä»¶æ„å»º\n",
      "============================================================\n",
      "\n",
      "ğŸ“ è¾“å…¥æ–‡ä»¶: normalized_tokens.json\n",
      "ğŸ“– ä»JSONæ–‡ä»¶åŠ è½½æ­£æ’è¡¨: normalized_tokens.json\n",
      "âœ… æˆåŠŸåŠ è½½JSONæ ¼å¼ï¼Œå…± 177065 ä¸ªæ–‡æ¡£\n",
      "   æ–‡æ¡£IDç¤ºä¾‹: ['Group 1005141', 'Group 1014534', 'Group 1017036', 'Group 1021017', 'Group 1030709']\n",
      "   è¯é¡¹ç¤ºä¾‹: ['subnat', 'purpose', 'discover', 'identify', 'plant']\n",
      "ğŸ”¨ æ„å»ºå¸¦è·³è¡¨æŒ‡é’ˆçš„å€’æ’ç´¢å¼•...\n",
      "  æ­¥éª¤1: æ„å»ºè¯æ±‡è¡¨...\n",
      "    è¯æ±‡è¡¨å¤§å°: 268574\n",
      "    è¯æ±‡è¡¨ç¤ºä¾‹: ['!!&nbsp;if', '!!limite', '!!synopsis', '!!welcome', '!).after', '!access', '!after', '!contact', '!either', '!for']\n",
      "  æ­¥éª¤2: åˆå§‹åŒ–æ–‡æ¡£æ’åºç³»ç»Ÿ...\n",
      "    æ–‡æ¡£æ•°é‡: 177065\n",
      "  æ­¥éª¤3: æ„å»ºåŸºç¡€å€’æ’åˆ—è¡¨...\n",
      "  æ­¥éª¤4: å¯¹å€’æ’åˆ—è¡¨æ’åº...\n",
      "  æ­¥éª¤5: æ·»åŠ è·³è¡¨æŒ‡é’ˆ...\n",
      "âœ… æ„å»ºå®Œæˆï¼š268574 ä¸ªè¯é¡¹ï¼Œ327296 ä¸ªè·³è¡¨æŒ‡é’ˆ\n",
      "\n",
      "ğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯:\n",
      "  è¯é¡¹æ•°é‡: 268574\n",
      "  æ–‡æ¡£æ•°é‡: 177065\n",
      "  å€’æ’è®°å½•æ€»æ•°: 8062721\n",
      "  è·³è¡¨æŒ‡é’ˆæ•°é‡: 327296\n",
      "  å¹³å‡å€’æ’åˆ—è¡¨é•¿åº¦: 30.02\n",
      "  è¯æ±‡è¡¨ç¤ºä¾‹: ['!!&nbsp;if', '!!limite', '!!synopsis', '!!welcome', '!).after', '!access', '!after', '!contact', '!either', '!for']\n",
      "\n",
      "========================================\n",
      "\n",
      "ğŸ’¾ ä¿å­˜å€’æ’ç´¢å¼•åˆ°: inverted_index.json\n",
      "âœ… å€’æ’ç´¢å¼•ä¿å­˜æˆåŠŸ\n",
      "\n",
      "âœ… å€’æ’ç´¢å¼•æ„å»ºæµç¨‹å®Œæˆ!\n",
      "   è¾“å…¥: normalized_tokens.json (JSONæ­£æ’è¡¨)\n",
      "   è¾“å‡º: inverted_index.json (JSONå€’æ’ç´¢å¼•)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Any, Set\n",
    "\n",
    "class ImprovedInvertedIndex:\n",
    "    \"\"\"\n",
    "    æ”¹è¿›çš„å€’æ’ç´¢å¼•ç±»ï¼Œç›´æ¥ä»JSONæ–‡ä»¶è¯»å–å·²å¤„ç†çš„æ­£æ’è¡¨æ•°æ®\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # è¯æ±‡è¡¨ï¼šè¯é¡¹ -> è¯é¡¹ID\n",
    "        self.vocabulary = {}\n",
    "        # å€’æ’åˆ—è¡¨ï¼šè¯é¡¹ID -> [(æ–‡æ¡£ID, è¯é¢‘, è·³è¡¨æŒ‡é’ˆ), ...]\n",
    "        self.inverted_lists = {}\n",
    "        # æ–‡æ¡£é•¿åº¦ï¼šæ–‡æ¡£ID -> æ–‡æ¡£ä¸­çš„è¯é¡¹æ•°é‡\n",
    "        self.doc_lengths = {}\n",
    "        # æ–‡æ¡£IDåˆ°æ’åºé”®çš„æ˜ å°„ï¼šæ–‡æ¡£ID -> æ’åºé”®ï¼ˆç”¨äºæ­£ç¡®æ¯”è¾ƒæ–‡æ¡£é¡ºåºï¼‰\n",
    "        self.doc_sort_keys = {}\n",
    "        # æ’åºé”®åˆ°æ–‡æ¡£IDçš„æ˜ å°„ï¼šæ’åºé”® -> æ–‡æ¡£IDï¼ˆç”¨äºåå‘æŸ¥æ‰¾ï¼‰\n",
    "        self.sort_key_to_doc = {}\n",
    "        # ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ’åºé”®\n",
    "        self.next_sort_key = 0\n",
    "    \n",
    "    def load_documents_from_json(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        ä»JSONæ–‡ä»¶åŠ è½½æ­£æ’è¡¨æ–‡æ¡£æ•°æ®\n",
    "        \n",
    "        å‚æ•°:\n",
    "            file_path: JSONæ­£æ’è¡¨æ–‡ä»¶è·¯å¾„\n",
    "            \n",
    "        è¿”å›:\n",
    "            æ–‡æ¡£å­—å…¸: {æ–‡æ¡£ID: [è¯é¡¹åˆ—è¡¨]}\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“– ä»JSONæ–‡ä»¶åŠ è½½æ­£æ’è¡¨: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                documents = json.load(f)\n",
    "            \n",
    "            # éªŒè¯æ•°æ®æ ¼å¼\n",
    "            if not isinstance(documents, dict):\n",
    "                raise ValueError(\"JSONæ–‡ä»¶é¡¶å±‚ç»“æ„å¿…é¡»æ˜¯å­—å…¸\")\n",
    "            \n",
    "            # éªŒè¯æ¯ä¸ªæ–‡æ¡£çš„è¯é¡¹åˆ—è¡¨\n",
    "            for doc_id, tokens in documents.items():\n",
    "                if not isinstance(tokens, list) or not all(isinstance(token, str) for token in tokens):\n",
    "                    raise ValueError(f\"æ–‡æ¡£ '{doc_id}' çš„å€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨\")\n",
    "            \n",
    "            print(f\"âœ… æˆåŠŸåŠ è½½JSONæ ¼å¼ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£\")\n",
    "            print(f\"   æ–‡æ¡£IDç¤ºä¾‹: {list(documents.keys())[:5]}\")\n",
    "            print(f\"   è¯é¡¹ç¤ºä¾‹: {list(documents.values())[0][:5] if documents else 'ç©º'}\")\n",
    "            \n",
    "            return documents\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ JSONæ–‡ä»¶åŠ è½½å¤±è´¥: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def build_index_from_file(self, file_path: str):\n",
    "        \"\"\"\n",
    "        ä»JSONæ­£æ’è¡¨æ–‡ä»¶æ„å»ºå€’æ’ç´¢å¼•\n",
    "        \n",
    "        æ­¥éª¤:\n",
    "        1. ä»æ–‡ä»¶åŠ è½½æ­£æ’è¡¨æ•°æ®\n",
    "        2. æ„å»ºå€’æ’ç´¢å¼•\n",
    "        3. æ·»åŠ è·³è¡¨æŒ‡é’ˆä¼˜åŒ–\n",
    "        \n",
    "        å‚æ•°:\n",
    "            file_path: JSONæ­£æ’è¡¨æ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        # æ­¥éª¤1: åŠ è½½æ­£æ’è¡¨æ•°æ®\n",
    "        documents = self.load_documents_from_json(file_path)\n",
    "        if not documents:\n",
    "            print(\"âŒ æ— æ³•åŠ è½½æ–‡æ¡£æ•°æ®ï¼Œç´¢å¼•æ„å»ºç»ˆæ­¢\")\n",
    "            return False\n",
    "        \n",
    "        # æ­¥éª¤2: æ„å»ºå€’æ’ç´¢å¼•\n",
    "        self._build_index_with_skips(documents)\n",
    "        return True\n",
    "    \n",
    "    def _get_doc_sort_key(self, doc_id: str) -> int:\n",
    "        \"\"\"ä¸ºæ–‡æ¡£IDç”Ÿæˆæ’åºé”®\"\"\"\n",
    "        if doc_id not in self.doc_sort_keys:\n",
    "            self.doc_sort_keys[doc_id] = self.next_sort_key\n",
    "            self.sort_key_to_doc[self.next_sort_key] = doc_id\n",
    "            self.next_sort_key += 1\n",
    "        return self.doc_sort_keys[doc_id]\n",
    "    \n",
    "    def _get_doc_id_from_sort_key(self, sort_key: int) -> str:\n",
    "        \"\"\"ä»æ’åºé”®è·å–æ–‡æ¡£ID\"\"\"\n",
    "        return self.sort_key_to_doc.get(sort_key, \"\")\n",
    "    \n",
    "    def _compare_docs(self, doc1: str, doc2: str) -> int:\n",
    "        \"\"\"æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£çš„é¡ºåº\"\"\"\n",
    "        key1 = self._get_doc_sort_key(doc1)\n",
    "        key2 = self._get_doc_sort_key(doc2)\n",
    "        if key1 < key2:\n",
    "            return -1\n",
    "        elif key1 > key2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def _build_index_with_skips(self, token_dict: Dict[str, List[str]]):\n",
    "        \"\"\"æ„å»ºå¸¦è·³è¡¨æŒ‡é’ˆçš„å€’æ’ç´¢å¼•\"\"\"\n",
    "        print(\"ğŸ”¨ æ„å»ºå¸¦è·³è¡¨æŒ‡é’ˆçš„å€’æ’ç´¢å¼•...\")\n",
    "        \n",
    "        # æ­¥éª¤1: æ„å»ºè¯æ±‡è¡¨\n",
    "        print(\"  æ­¥éª¤1: æ„å»ºè¯æ±‡è¡¨...\")\n",
    "        all_tokens = set()\n",
    "        for doc_tokens in token_dict.values():\n",
    "            all_tokens.update(doc_tokens)\n",
    "        \n",
    "        sorted_tokens = sorted(all_tokens)\n",
    "        self.vocabulary = {token: idx for idx, token in enumerate(sorted_tokens)}\n",
    "        print(f\"    è¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}\")\n",
    "        print(f\"    è¯æ±‡è¡¨ç¤ºä¾‹: {list(self.vocabulary.keys())[:10]}\")\n",
    "        \n",
    "        # æ­¥éª¤2: åˆå§‹åŒ–æ–‡æ¡£æ’åºç³»ç»Ÿ\n",
    "        print(\"  æ­¥éª¤2: åˆå§‹åŒ–æ–‡æ¡£æ’åºç³»ç»Ÿ...\")\n",
    "        for doc_id in token_dict.keys():\n",
    "            self._get_doc_sort_key(doc_id)\n",
    "            self.doc_lengths[doc_id] = len(token_dict[doc_id])\n",
    "        print(f\"    æ–‡æ¡£æ•°é‡: {len(token_dict)}\")\n",
    "        \n",
    "        # æ­¥éª¤3: æ„å»ºåŸºç¡€å€’æ’åˆ—è¡¨\n",
    "        print(\"  æ­¥éª¤3: æ„å»ºåŸºç¡€å€’æ’åˆ—è¡¨...\")\n",
    "        for doc_id, tokens in token_dict.items():\n",
    "            term_freq = {}\n",
    "            for token in tokens:\n",
    "                term_id = self.vocabulary[token]\n",
    "                term_freq[term_id] = term_freq.get(term_id, 0) + 1\n",
    "            \n",
    "            for term_id, freq in term_freq.items():\n",
    "                if term_id not in self.inverted_lists:\n",
    "                    self.inverted_lists[term_id] = []\n",
    "                self.inverted_lists[term_id].append((doc_id, freq, -1))\n",
    "        \n",
    "        # æ­¥éª¤4: å¯¹æ¯ä¸ªå€’æ’åˆ—è¡¨æ’åº\n",
    "        print(\"  æ­¥éª¤4: å¯¹å€’æ’åˆ—è¡¨æ’åº...\")\n",
    "        for term_id, postings in self.inverted_lists.items():\n",
    "            postings.sort(key=lambda x: self._get_doc_sort_key(x[0]))\n",
    "        \n",
    "        # æ­¥éª¤5: ä¸ºé•¿å€’æ’åˆ—è¡¨æ·»åŠ è·³è¡¨æŒ‡é’ˆ\n",
    "        print(\"  æ­¥éª¤5: æ·»åŠ è·³è¡¨æŒ‡é’ˆ...\")\n",
    "        skip_count = 0\n",
    "        for term_id, postings in self.inverted_lists.items():\n",
    "            if len(postings) >= 4:\n",
    "                skip_count += self._add_skip_pointers(term_id, postings)\n",
    "        \n",
    "        print(f\"âœ… æ„å»ºå®Œæˆï¼š{len(self.inverted_lists)} ä¸ªè¯é¡¹ï¼Œ{skip_count} ä¸ªè·³è¡¨æŒ‡é’ˆ\")\n",
    "    \n",
    "    def _add_skip_pointers(self, term_id: int, postings: List[Tuple]) -> int:\n",
    "        \"\"\"ä¸ºå€’æ’åˆ—è¡¨æ·»åŠ è·³è¡¨æŒ‡é’ˆ\"\"\"\n",
    "        n = len(postings)\n",
    "        skip_interval = int(math.sqrt(n))\n",
    "        skip_count = 0\n",
    "        \n",
    "        for i in range(0, n, skip_interval):\n",
    "            if i + skip_interval < n:\n",
    "                doc_id, freq, _ = postings[i]\n",
    "                postings[i] = (doc_id, freq, i + skip_interval)\n",
    "                skip_count += 1\n",
    "        \n",
    "        return skip_count\n",
    "    \n",
    "    def search_with_skips_improved(self, query_terms: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"ä½¿ç”¨è·³è¡¨æŒ‡é’ˆçš„æ”¹è¿›æœç´¢ç®—æ³•\"\"\"\n",
    "        print(f\"\\nğŸ” æœç´¢æŸ¥è¯¢: {query_terms}\")\n",
    "        \n",
    "        if not query_terms:\n",
    "            return {\"documents\": [], \"stats\": {\"message\": \"ç©ºæŸ¥è¯¢\"}}\n",
    "        \n",
    "        term_ids = []\n",
    "        for term in query_terms:\n",
    "            if term in self.vocabulary:\n",
    "                term_ids.append(self.vocabulary[term])\n",
    "        \n",
    "        if not term_ids:\n",
    "            return {\"documents\": [], \"stats\": {\"message\": \"æœªæ‰¾åˆ°åŒ¹é…çš„è¯é¡¹\"}}\n",
    "        \n",
    "        print(f\"  æ‰¾åˆ° {len(term_ids)} ä¸ªåŒ¹é…è¯é¡¹\")\n",
    "        \n",
    "        term_ids.sort(key=lambda tid: len(self.inverted_lists.get(tid, [])))\n",
    "        \n",
    "        first_term_id = term_ids[0]\n",
    "        result_docs = set(self._get_doc_ids(first_term_id))\n",
    "        print(f\"  åˆå§‹åˆ—è¡¨é•¿åº¦: {len(result_docs)}\")\n",
    "        \n",
    "        for i, term_id in enumerate(term_ids[1:], 1):\n",
    "            current_docs = self._get_doc_ids(term_id)\n",
    "            print(f\"  ä¸ç¬¬{i+1}ä¸ªåˆ—è¡¨æ±‚äº¤é›†ï¼Œé•¿åº¦: {len(current_docs)}\")\n",
    "            \n",
    "            result_docs = self._intersect_with_skips_improved(\n",
    "                list(result_docs), current_docs, term_id\n",
    "            )\n",
    "            print(f\"    äº¤é›†åé•¿åº¦: {len(result_docs)}\")\n",
    "            \n",
    "            if not result_docs:\n",
    "                break\n",
    "        \n",
    "        sorted_docs = sorted(list(result_docs), key=lambda x: self._get_doc_sort_key(x))\n",
    "        \n",
    "        return {\n",
    "            \"documents\": sorted_docs,\n",
    "            \"stats\": {\n",
    "                \"query_terms\": query_terms,\n",
    "                \"matched_terms\": len(term_ids),\n",
    "                \"result_count\": len(result_docs),\n",
    "                \"searched_lists\": len(term_ids)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _get_doc_ids(self, term_id: int) -> List[str]:\n",
    "        \"\"\"è·å–è¯é¡¹å¯¹åº”çš„æ–‡æ¡£IDåˆ—è¡¨\"\"\"\n",
    "        if term_id not in self.inverted_lists:\n",
    "            return []\n",
    "        return [doc_id for doc_id, _, _ in self.inverted_lists[term_id]]\n",
    "    \n",
    "    def _intersect_with_skips_improved(self, list1: List[str], list2: List[str], term2_id: int) -> Set[str]:\n",
    "        \"\"\"ä½¿ç”¨è·³è¡¨æŒ‡é’ˆæ±‚ä¸¤ä¸ªæ–‡æ¡£åˆ—è¡¨çš„äº¤é›†\"\"\"\n",
    "        result = set()\n",
    "        i = j = 0\n",
    "        postings2 = self.inverted_lists[term2_id]\n",
    "        \n",
    "        while i < len(list1) and j < len(list2):\n",
    "            doc1 = list1[i]\n",
    "            doc2 = list2[j]\n",
    "            \n",
    "            cmp_result = self._compare_docs(doc1, doc2)\n",
    "            \n",
    "            if cmp_result == 0:\n",
    "                result.add(doc1)\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif cmp_result < 0:\n",
    "                i += 1\n",
    "            else:\n",
    "                new_j = self._skip_forward(postings2, j, doc1)\n",
    "                if new_j > j:\n",
    "                    j = new_j\n",
    "                else:\n",
    "                    j += 1\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _skip_forward(self, postings: List[Tuple], current_pos: int, target_doc: str) -> int:\n",
    "        \"\"\"ä½¿ç”¨è·³è¡¨æŒ‡é’ˆå‘å‰è·³è½¬\"\"\"\n",
    "        if current_pos >= len(postings):\n",
    "            return current_pos\n",
    "        \n",
    "        _, _, skip_ptr = postings[current_pos]\n",
    "        if skip_ptr != -1 and skip_ptr < len(postings):\n",
    "            skip_doc_id, _, _ = postings[skip_ptr]\n",
    "            \n",
    "            if self._compare_docs(skip_doc_id, target_doc) <= 0:\n",
    "                further_skip = self._skip_forward(postings, skip_ptr, target_doc)\n",
    "                if further_skip > skip_ptr:\n",
    "                    return further_skip\n",
    "                return skip_ptr\n",
    "        \n",
    "        return current_pos\n",
    "    \n",
    "    def save_inverted_index(self, output_path: str):\n",
    "        \"\"\"\n",
    "        å°†å€’æ’ç´¢å¼•ä¿å­˜åˆ°JSONæ–‡ä»¶\n",
    "        \n",
    "        å‚æ•°:\n",
    "            output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ’¾ ä¿å­˜å€’æ’ç´¢å¼•åˆ°: {output_path}\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('{\\n')\n",
    "                f.write('  \"vocabulary\": ')\n",
    "                json.dump(self.vocabulary, f, indent=2, ensure_ascii=False, separators=(',', ': '))\n",
    "                f.write(',\\n')\n",
    "                \n",
    "                f.write('  \"inverted_lists\": {\\n')\n",
    "                term_ids = sorted(self.inverted_lists.keys(), key=int)\n",
    "                for i, term_id in enumerate(term_ids):\n",
    "                    f.write(f'    \"{term_id}\": [')\n",
    "                    postings = self.inverted_lists[term_id]\n",
    "                    for j, (doc_id, freq, skip_ptr) in enumerate(postings):\n",
    "                        if j > 0:\n",
    "                            f.write(', ')\n",
    "                        f.write(f'[\"{doc_id}\", {freq}, {skip_ptr}]')\n",
    "                    f.write(']')\n",
    "                    if i < len(term_ids) - 1:\n",
    "                        f.write(',')\n",
    "                    f.write('\\n')\n",
    "                f.write('  },\\n')\n",
    "                \n",
    "                f.write('  \"doc_lengths\": ')\n",
    "                json.dump(self.doc_lengths, f, indent=2, ensure_ascii=False, separators=(',', ': '))\n",
    "                f.write(',\\n')\n",
    "                \n",
    "                f.write('  \"doc_sort_keys\": ')\n",
    "                json.dump(self.doc_sort_keys, f, indent=2, ensure_ascii=False, separators=(',', ': '))\n",
    "                f.write(',\\n')\n",
    "                \n",
    "                f.write('  \"sort_key_to_doc\": ')\n",
    "                json.dump(self.sort_key_to_doc, f, indent=2, ensure_ascii=False, separators=(',', ': '))\n",
    "                f.write(',\\n')\n",
    "                \n",
    "                f.write(f'  \"next_sort_key\": {self.next_sort_key}\\n')\n",
    "                f.write('}\\n')\n",
    "                \n",
    "            print(\"âœ… å€’æ’ç´¢å¼•ä¿å­˜æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ä¿å­˜å¤±è´¥: {e}\")\n",
    "    \n",
    "    def print_index_statistics(self):\n",
    "        \"\"\"æ‰“å°ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        print(\"\\nğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "        print(f\"  è¯é¡¹æ•°é‡: {len(self.vocabulary)}\")\n",
    "        print(f\"  æ–‡æ¡£æ•°é‡: {len(self.doc_lengths)}\")\n",
    "        \n",
    "        total_postings = sum(len(postings) for postings in self.inverted_lists.values())\n",
    "        print(f\"  å€’æ’è®°å½•æ€»æ•°: {total_postings}\")\n",
    "        \n",
    "        skip_count = self._count_skips()\n",
    "        print(f\"  è·³è¡¨æŒ‡é’ˆæ•°é‡: {skip_count}\")\n",
    "        \n",
    "        if self.inverted_lists:\n",
    "            avg_length = total_postings / len(self.inverted_lists)\n",
    "            print(f\"  å¹³å‡å€’æ’åˆ—è¡¨é•¿åº¦: {avg_length:.2f}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºè¯æ±‡è¡¨ç¤ºä¾‹\n",
    "        if self.vocabulary:\n",
    "            sample_terms = list(self.vocabulary.keys())[:10]\n",
    "            print(f\"  è¯æ±‡è¡¨ç¤ºä¾‹: {sample_terms}\")\n",
    "    \n",
    "    def print_skip_structure(self, term: str):\n",
    "        \"\"\"æ‰“å°è¯é¡¹çš„è·³è¡¨ç»“æ„\"\"\"\n",
    "        if term not in self.vocabulary:\n",
    "            print(f\"è¯é¡¹ '{term}' ä¸å­˜åœ¨\")\n",
    "            return\n",
    "        \n",
    "        term_id = self.vocabulary[term]\n",
    "        if term_id not in self.inverted_lists:\n",
    "            print(f\"è¯é¡¹ '{term}' æ— å€’æ’åˆ—è¡¨\")\n",
    "            return\n",
    "        \n",
    "        postings = self.inverted_lists[term_id]\n",
    "        print(f\"\\nğŸ“‹ è¯é¡¹ '{term}' çš„è·³è¡¨ç»“æ„ (å…±{len(postings)}ä¸ªæ–‡æ¡£):\")\n",
    "        \n",
    "        for i, (doc_id, freq, skip_ptr) in enumerate(postings):\n",
    "            skip_info = f\" â†’ [{skip_ptr}]\" if skip_ptr != -1 else \"\"\n",
    "            sort_key = self._get_doc_sort_key(doc_id)\n",
    "            print(f\"  [{i:2d}] æ–‡æ¡£: {doc_id:15s} (æ’åºé”®:{sort_key:2d}), é¢‘ç‡: {freq}{skip_info}\")\n",
    "    \n",
    "    def _count_skips(self) -> int:\n",
    "        \"\"\"ç»Ÿè®¡è·³è¡¨æŒ‡é’ˆæ•°é‡\"\"\"\n",
    "        count = 0\n",
    "        for postings in self.inverted_lists.values():\n",
    "            for _, _, skip_ptr in postings:\n",
    "                if skip_ptr != -1:\n",
    "                    count += 1\n",
    "        return count\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°ï¼šä»JSONæ­£æ’è¡¨æ–‡ä»¶æ„å»ºå€’æ’ç´¢å¼•\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸš€ å€’æ’ç´¢å¼•æ„å»ºç³»ç»Ÿ - ä»JSONæ­£æ’è¡¨æ–‡ä»¶æ„å»º\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åˆ›å»ºå€’æ’ç´¢å¼•å®ä¾‹\n",
    "    index = ImprovedInvertedIndex()\n",
    "    \n",
    "    # æ–‡ä»¶è·¯å¾„é…ç½® - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è¿™äº›è·¯å¾„\n",
    "    forward_file_path = \"normalized_tokens.json\"    # è¾“å…¥çš„JSONæ­£æ’è¡¨æ–‡ä»¶è·¯å¾„\n",
    "    inverted_index_path = \"inverted_index.json\" # è¾“å‡ºçš„å€’æ’ç´¢å¼•JSONæ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    # ä»JSONæ–‡ä»¶æ„å»ºå€’æ’ç´¢å¼•\n",
    "    print(f\"\\nğŸ“ è¾“å…¥æ–‡ä»¶: {forward_file_path}\")\n",
    "    success = index.build_index_from_file(forward_file_path)\n",
    "    \n",
    "    if not success:\n",
    "        print(\"âŒ ç´¢å¼•æ„å»ºå¤±è´¥ï¼Œç¨‹åºé€€å‡º\")\n",
    "        return\n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "    index.print_index_statistics()\n",
    "    \n",
    "    # ä¿å­˜å€’æ’ç´¢å¼•åˆ°JSONæ–‡ä»¶\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    index.save_inverted_index(inverted_index_path)\n",
    "    \n",
    "    print(\"\\nâœ… å€’æ’ç´¢å¼•æ„å»ºæµç¨‹å®Œæˆ!\")\n",
    "    print(f\"   è¾“å…¥: {forward_file_path} (JSONæ­£æ’è¡¨)\")\n",
    "    print(f\"   è¾“å‡º: {inverted_index_path} (JSONå€’æ’ç´¢å¼•)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
